# REPORTS — What We Measure and How to Act

This repo produces several report artifacts under `data/reports/<run-id>/`. The goal is to turn those into a short list of **next actions** (what to fix next, safely).

Canonical entry point:
```bash
python3 scripts/actions.py workflow.improve --help
```

## 1) Truth evaluation (`eval.truth/`)
Generated by `eval.truth` (wrapper) / `msl phase3-baseline` (engine).

### `eval.truth/baseline_decoder_scorecard.csv`
Per-brand summary table. Key columns:
- `YearCoveragePct`: % of rows with serial where a year was decoded
- `YearAccuracyPct`: % correct among rows where year was decoded *and* labeled
- `AnyAttributesCoveragePct`: % of rows with model where any attribute was decoded
- `NominalCapacityTonsCoveragePct`: % of rows with model where tons was decoded

How to act:
- Low `YearCoveragePct` + high `KnownYearN`: missing serial rules for that brand. Prioritize mining/documentation.
- Low `YearAccuracyPct`: rule collision/overmatch. Prioritize tightening regex or adding discriminators (e.g., length constraints).
- Low attribute coverage: missing attribute rules (or intentionally conservative rules).

### `eval.truth/next_targets.md` (also copied to run root as `NEXT_TARGETS.md`)
A human summary of:
- Top year coverage gaps
- Top wrong-year volume
- Top attribute gaps

How to act:
- Treat this as the default “backlog generator” for the next iteration.
- Pick the top 1–3 brands by volume, not by count of gaps.

### `eval.truth/baseline_decoder_output.csv`
Row-level baseline output (canonicalized input + decode results).

How to act:
- Filter by a brand from `next_targets.md`, then inspect:
  - `MatchedSerialStyle`, `ManufactureYear`, `ManufactureDateEvidence`, `ManufactureDateSourceURL`
- For wrong-year cases, look for a competing style that also matches (collision).

### `eval.truth/training_data_profile.md`
Profile of missingness and top brands/types in the labeled dataset.

How to act:
- If a brand is high-volume but has low `KnownYearN`, it’s hard to mine year rules safely—collect better labels first.

## 2) Candidate mining (`mine.rules/` + `data/rules_discovered/<run-id>/candidates/`)
Generated by `mine.rules` (wrapper) / `msl phase3-mine` (engine).

### `data/rules_discovered/<run-id>/candidates/SerialDecodeRule.candidates.jsonl`
Serial-year candidate rules mined from labeled data.

What’s inside (typical):
- `brand`, `serial_regex`, `date_fields`
- Metrics (often embedded as fields or in `style_name`/notes)

How to act:
- Use candidates as proposals; never assume they are correct without audit + no-regression checks.

### `data/rules_discovered/<run-id>/candidates/AttributeDecodeRule.candidates.jsonl`
Attribute candidates (currently capacity-focused).

How to act:
- Prefer candidates with high accuracy and low cross-brand match rate (see audit outputs).

## 3) Candidate audit (`eval.candidates/`)
Generated by `eval.candidates` (wrapper) / `msl phase3-audit` (engine).

### `eval.candidates/holdout_validation_results.csv`
Per-candidate metrics:
- `accuracy_on_matches`
- `coverage_on_truth`
- `matched_rows` / `truth_rows`

How to act:
- High accuracy but low coverage: candidate is “narrow” (still useful, but expect small gains).
- High coverage but lower accuracy: collision risk; do not promote without tightening.

### `eval.candidates/false_positive_audit.csv` (attributes)
Collision signal:
- `other_brand_match_rate`

How to act:
- Treat high `other_brand_match_rate` as a hard “no” unless you add additional gating (equipment type, brand normalization, tighter regex).

## 4) Promotion outputs (`ruleset.promote/` + run root)
Generated when promotion happens.

### `ruleset.promote/ruleset_diff.md` + `ruleset_diff.json`
Row-key diff of what changed vs base ruleset.

How to act:
- Confirm that the promotion changed what you expected (brands and rule types).
- Large diffs with no metric movement suggest the candidates aren’t relevant to the dataset slice.

### `delta_scorecard.csv` (only when `workflow.improve --promote`)
Per-brand before/after deltas for year coverage/accuracy.

How to act:
- Positive deltas on high-volume brands = good promotion.
- Negative deltas should block promotion in a future “no-regression” gate.

## 5) Making reports actionable (recommended loop)
1. Run `workflow.improve` (no promote) and read `NEXT_TARGETS.md`.
2. Inspect `baseline_decoder_output.csv` for the top brand(s) and classify failures:
   - missing rules (coverage)
   - collisions (accuracy)
   - missing labels (can’t mine safely)
3. Mine candidates + audit.
4. Promote (audit-gated by default; use `--promote-all` only intentionally).
5. Re-run truth and confirm deltas (`delta_scorecard.csv`).

