#!/usr/bin/env python3
from __future__ import annotations

import argparse
import csv
import json
import sys
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any


REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from scripts.hvacexport_runlib import create_run_dir
from scripts.hvacexport_runlib import utc_compact_ts


def _read_csv_rows(path: Path) -> list[dict[str, str]]:
    with path.open("r", newline="", encoding="utf-8") as f:
        return list(csv.DictReader(f))


def _jsonl_write(path: Path, objs: list[dict[str, Any]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    lines = [json.dumps(o, ensure_ascii=False) for o in objs]
    path.write_text("\n".join(lines) + ("\n" if lines else ""), encoding="utf-8")


@dataclass(frozen=True)
class GroupKey:
    brand: str
    equipment_type: str
    model_regex: str
    start: int
    end: int


def main(argv: list[str] | None = None) -> int:
    ap = argparse.ArgumentParser(
        prog="hvacexport_generate_capacity_from_code_suggestions",
        description="Generate NominalCapacityTons candidates from capacity_code_suggestions.csv (audit-only; no ruleset integration).",
    )
    ap.add_argument("--snapshot-id", required=True)
    ap.add_argument("--alignment-run-id", required=True, help="Run id under derived/runs/ that contains capacity_code_suggestions.csv")
    ap.add_argument("--hvacexport-root", default=str(REPO_ROOT / "data" / "external_sources" / "hvacexport"))
    ap.add_argument("--run-id", default="", help="Output run folder name under derived/runs/ (default: autogenerated)")
    ap.add_argument(
        "--source-attributes",
        default="HVACExport_CoolingCode,HVACExport_CoolingTonCode",
        help="Comma-separated list of staged code attributes to convert into capacity mappings",
    )
    ap.add_argument("--min-support", type=int, default=10)
    ap.add_argument("--min-purity", type=float, default=95.0)
    ap.add_argument("--require-unique-truth", action="store_true", help="Require UniqueTruthTonsN == 1 (recommended)")
    ap.add_argument("--brands", default="", help="Optional comma-separated BrandNormalized filter (e.g., TRANE,LENNOX,YORK/JCI)")
    args = ap.parse_args(argv)

    snap_dir = Path(args.hvacexport_root) / str(args.snapshot_id)
    align_dir = snap_dir / "derived" / "runs" / str(args.alignment_run_id)
    suggestions_csv = align_dir / "capacity_code_suggestions.csv"
    if not suggestions_csv.exists():
        raise SystemExit(f"Missing suggestions file: {suggestions_csv}")

    source_attrs = {s.strip() for s in str(args.source_attributes).split(",") if s.strip()}
    min_support = int(args.min_support)
    min_purity = float(args.min_purity)
    brands_filter = {s.strip() for s in str(args.brands).split(",") if s.strip()}

    rows = _read_csv_rows(suggestions_csv)
    kept = 0
    dropped = 0
    drop_reasons: dict[str, int] = defaultdict(int)

    grouped: dict[GroupKey, dict[str, float]] = defaultdict(dict)
    for r in rows:
        attr = (r.get("AttributeName") or "").strip()
        if attr not in source_attrs:
            dropped += 1
            drop_reasons["attr_not_selected"] += 1
            continue

        try:
            support = int((r.get("SupportN") or "").strip() or "0")
        except Exception:
            support = 0
        try:
            purity = float((r.get("PurityPct") or "").strip() or "0")
        except Exception:
            purity = 0.0
        try:
            unique_truth = int((r.get("UniqueTruthTonsN") or "").strip() or "0")
        except Exception:
            unique_truth = 0

        if support < min_support:
            dropped += 1
            drop_reasons["low_support"] += 1
            continue
        if purity < min_purity:
            dropped += 1
            drop_reasons["low_purity"] += 1
            continue
        if args.require_unique_truth and unique_truth != 1:
            dropped += 1
            drop_reasons["non_unique_truth"] += 1
            continue

        brand = (r.get("BrandNormalized") or "").strip()
        if brands_filter and brand not in brands_filter:
            dropped += 1
            drop_reasons["brand_not_selected"] += 1
            continue
        equipment_type = (r.get("EquipmentType") or "").strip()
        model_regex = (r.get("RuleModelRegex") or "").strip()
        code = (r.get("Code") or "").strip()
        most_tons = (r.get("MostCommonTons") or "").strip()

        if not brand or not model_regex or not code or not most_tons:
            dropped += 1
            drop_reasons["missing_fields"] += 1
            continue

        try:
            start = int((r.get("RuleStartPos") or "").strip())
            end = int((r.get("RuleEndPos") or "").strip())
        except Exception:
            dropped += 1
            drop_reasons["bad_positions"] += 1
            continue
        try:
            tons_v = float(most_tons)
        except Exception:
            dropped += 1
            drop_reasons["bad_tons_value"] += 1
            continue

        gk = GroupKey(brand=brand, equipment_type=equipment_type, model_regex=model_regex, start=start, end=end)
        # If conflicts happen (same code => different tons), drop that code for this group.
        if code in grouped[gk] and grouped[gk][code] != tons_v:
            grouped[gk].pop(code, None)
            drop_reasons["conflicting_code_mapping"] += 1
            continue
        grouped[gk][code] = tons_v
        kept += 1

    out_run_id = (args.run_id or "").strip() or f"capacity_from_codes__{utc_compact_ts()}"
    out_dir = create_run_dir(snap_dir, run_id=out_run_id)
    out_path = out_dir / "AttributeDecodeRule.hvacexport.capacity_from_codes.candidates.jsonl"

    out_objs: list[dict[str, Any]] = []
    for gk, mapping in sorted(grouped.items(), key=lambda kv: (kv[0].brand, kv[0].equipment_type, kv[0].model_regex, kv[0].start, kv[0].end)):
        if not mapping:
            continue
        equipment_types = [gk.equipment_type] if gk.equipment_type else []
        out_objs.append(
            {
                "rule_type": "decode",
                "brand": gk.brand,
                "attribute_name": "NominalCapacityTons",
                "model_regex": gk.model_regex,
                "equipment_types": equipment_types,
                "value_extraction": {
                    "data_type": "Number",
                    "positions": {"start": gk.start, "end": gk.end},
                    "mapping": mapping,
                },
                "units": "tons",
                "examples": [],
                "limitations": (
                    "Derived from labeled SDI truth via hvacexport staged code attributes; audit-only. "
                    f"Filters: attrs={sorted(source_attrs)}, min_support={min_support}, min_purity={min_purity}, require_unique_truth={bool(args.require_unique_truth)}. "
                    "Mapping is per (brand,equipment_type,model_regex,positions) to keep it auditable."
                ),
                "evidence_excerpt": f"capacity_code_suggestions:{args.alignment_run_id}",
                "source_url": f"hvacexport_code_suggestions:{snap_dir.name}:{args.alignment_run_id}",
                "retrieved_on": "",
                "image_urls": [],
            }
        )

    _jsonl_write(out_path, out_objs)
    summary = {
        "snapshot_id": snap_dir.name,
        "alignment_run_id": str(args.alignment_run_id),
        "rows_in": len(rows),
        "rows_kept": kept,
        "rows_dropped": dropped,
        "drop_reasons": dict(sorted(drop_reasons.items())),
        "rules_out_n": len(out_objs),
        "out_path": str(out_path.relative_to(snap_dir)),
    }
    (out_dir / "AttributeDecodeRule.hvacexport.capacity_from_codes.summary.json").write_text(json.dumps(summary, indent=2) + "\n", encoding="utf-8")
    (snap_dir / "derived" / "LATEST_CAPACITY_FROM_CODES.txt").write_text(str(out_path.relative_to(snap_dir)), encoding="utf-8")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
